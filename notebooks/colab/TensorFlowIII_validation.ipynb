{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlowIII-validation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "oSPtj1CtP5z8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QXvKqw7NDV21",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Read data, index by date"
      ]
    },
    {
      "metadata": {
        "id": "bzKPkoCXQAE4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fileurl = \"https://s3-eu-west-1.amazonaws.com/training180529/data/djia_close.csv\"\n",
        "df = pd.read_csv(fileurl, sep=',',header=0)\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df = df.sort_values(by= 'date').reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5jnovhniDbAb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Validation as random samples in the timeline"
      ]
    },
    {
      "metadata": {
        "id": "4mNMMXwiDi-B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train=df.sample(frac=2/3,random_state=200)\n",
        "test=df.drop(train.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UQ5a-WXtDi0f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(train.shape)\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "whIj-inlQRpX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(train.shape)\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1HfKrW9pQnzi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 2e-8\n",
        "training_steps = 50000\n",
        "losses = []\n",
        "errors = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    x = tf.constant(train.drop(columns=['date', 'JPM', 'DWDP', 'MMM']))\n",
        "#    x = tf.constant(train[['AXP']])\n",
        "    y = tf.constant(train[['JPM']])\n",
        "    \n",
        "    x_test = tf.constant(test.drop(columns=['date', 'JPM', 'DWDP', 'MMM']))\n",
        "    y_test = tf.constant(test[['JPM']])\n",
        "    \n",
        "    weights = tf.Variable(tf.random_normal([27, 1], 0, 1.0/30, dtype=tf.float64))\n",
        "    \n",
        "    b = tf.Variable(tf.random_normal([1], 0, 0.1, dtype=tf.float64))\n",
        "    \n",
        "    tf.global_variables_initializer().run()\n",
        "    yhat = x @ weights + b\n",
        "    yerror = tf.subtract(y,yhat)\n",
        "    \n",
        "    loss = tf.nn.l2_loss(yerror)\n",
        "    update_weights = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
        "    \n",
        "    yerror_test = x_test @ weights + b - y_test\n",
        "    loss_test = tf.nn.l2_loss(yerror_test)\n",
        "    \n",
        "    for _ in range(training_steps):\n",
        "        # Repeatedly run the operations, updating the TensorFlow variable.\n",
        "        sess.run(update_weights)\n",
        "        losses.append(math.sqrt(loss.eval()/train.shape[0]))\n",
        "        errors.append(math.sqrt(loss_test.eval()/test.shape[0]))\n",
        "        #print(\"{} vs {}\".format(loss_test.eval(), loss.eval()))\n",
        "    betas = weights.eval()\n",
        "    bias = b.eval()\n",
        "    yhat = yhat.eval()\n",
        "    y = y.eval()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xFYe4jhXQxBN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.yscale('log')\n",
        "plt.plot(range(0, training_steps), losses)\n",
        "plt.plot(range(0, training_steps), errors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dUSI106ASbgM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(errors[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VDagh22HE71b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ug8HT9SdFKBK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Validation as 3rd third of data in the timesline order"
      ]
    },
    {
      "metadata": {
        "id": "S_zQbIUQQGe5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "split_idx = int(2*df.shape[0]/3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dylVkjC7QNGv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train=df[:split_idx]#.sample(frac=0.8,random_state=200)\n",
        "test=df[split_idx:]#.drop(train.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X40yAQ83FQPG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 2e-8\n",
        "training_steps = 50000\n",
        "losses_1 = []\n",
        "errors_1 = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    x = tf.constant(train.drop(columns=['date', 'JPM', 'DWDP', 'MMM']))\n",
        "#    x = tf.constant(train[['AXP']])\n",
        "    y = tf.constant(train[['JPM']])\n",
        "    \n",
        "    x_test = tf.constant(test.drop(columns=['date', 'JPM', 'DWDP', 'MMM']))\n",
        "    y_test = tf.constant(test[['JPM']])\n",
        "    \n",
        "    weights = tf.Variable(tf.random_normal([27, 1], 0, 1.0/30, dtype=tf.float64))\n",
        "    \n",
        "    b = tf.Variable(tf.random_normal([1], 0, 0.1, dtype=tf.float64))\n",
        "    \n",
        "    tf.global_variables_initializer().run()\n",
        "    yhat = x @ weights + b\n",
        "    yerror = tf.subtract(yhat, y)\n",
        "    \n",
        "    loss = tf.nn.l2_loss(yerror)\n",
        "    update_weights = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
        "    \n",
        "    yerror_test = x_test @ weights + b - y_test\n",
        "    loss_test = tf.nn.l2_loss(yerror_test)\n",
        "    \n",
        "    for _ in range(training_steps):\n",
        "        # Repeatedly run the operations, updating the TensorFlow variable.\n",
        "        sess.run(update_weights)\n",
        "        losses_1.append(math.sqrt(loss.eval()/train.shape[0]))\n",
        "        errors_1.append(math.sqrt(loss_test.eval()/test.shape[0]))\n",
        "        #print(\"{} vs {}\".format(loss_test.eval(), loss.eval()))\n",
        "    betas = weights.eval()\n",
        "    bias = b.eval()\n",
        "    yhat = yhat.eval()\n",
        "    y = y.eval()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hql0mUODlJlm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Plot RMSE for both validation schemes, see how time-ordered validation proves less prone to overfitting as RMSE remains higher than pure random-picked validation samples"
      ]
    },
    {
      "metadata": {
        "id": "ecnvgB9HFkEy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.yscale('log')\n",
        "plt.plot(range(0, training_steps), losses, '#0000FF')\n",
        "plt.plot(range(0, training_steps), errors, '#00FFFF')\n",
        "plt.plot(range(0, training_steps), losses_1, '#FF0000')\n",
        "plt.plot(range(0, training_steps), errors_1, '#FFFF00')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G3EsqaWNH4XG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(np.min(losses))\n",
        "print(np.min(losses_1))\n",
        "print(np.min(errors))\n",
        "print(np.min(errors_1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ejX5DdjRH7hG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}